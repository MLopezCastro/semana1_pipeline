# Semana 1 â€“ Data Pipeline Project

Este proyecto forma parte del Bootcamp de Data Engineering.

## ğŸ“¦ Estructura

- `ejemplo.py`: archivo de prueba para herramientas de calidad de cÃ³digo.
- `venv/`: entorno virtual local (ignorado en Git).
- `requirements.txt`: dependencias instaladas.
- `.gitignore`: exclusiÃ³n de carpetas innecesarias.

## ğŸ› ï¸ Herramientas utilizadas

- Python 3.10+
- `black`, `flake8`, `isort`
- Git + GitHub
- VS Code

## ğŸš€ CÃ³mo empezar

```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt


---

# **ETL Pipeline â€“ Ventas (Bloques 1 a 6)**

## **ğŸ“Œ DescripciÃ³n General**

Este proyecto implementa un **pipeline ETL** para procesar y limpiar datos de ventas desde un archivo CSV, usando Python de forma modular y profesional.
La estructura se ha diseÃ±ado siguiendo buenas prÃ¡cticas de **Data Engineering**, con separaciÃ³n en etapas y scripts reutilizables.

---

## **1ï¸âƒ£ Bloque 1 â€“ Estructura base del proyecto**

* Se crea la **estructura de carpetas**:

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â””â”€â”€ output/
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ load.py
â”‚   â”œâ”€â”€ save.py
â”‚   â”œâ”€â”€ transform.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ main_ventas_cli.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

* **Objetivo**: Separar la lÃ³gica de negocio, entrada/salida y utilidades en mÃ³dulos.
* **Archivos creados**:

  * `pipeline/load.py`: FunciÃ³n para cargar CSV (`pandas.read_csv`).
  * `pipeline/save.py`: FunciÃ³n para guardar CSV (`DataFrame.to_csv`).
  * `pipeline/transform.py`: Funciones de limpieza y transformaciÃ³n.

---

## **2ï¸âƒ£ Bloque 2 â€“ Funciones de transformaciÃ³n**

Funciones implementadas en `pipeline/transform.py`:

* `clean_column_names(df)`: Normaliza nombres de columnas (minÃºsculas, sin espacios).
* `drop_nulls(df)`: Elimina filas con valores nulos.
* `filter_positive_values(df, col)`: Filtra filas con valores > 0 en una columna.
* `drop_duplicates_by_column(df, col)`: Elimina duplicados basados en una columna.
* `normalize_column_0_100(df, col)`: Normaliza valores a escala 0â€“100.
* `uppercase_column(df, col)`: Convierte a mayÃºsculas los valores de una columna.

---

## **3ï¸âƒ£ Bloque 3 â€“ Pipeline local bÃ¡sico**

* Se crea un `main_ventas.py` bÃ¡sico para encadenar las funciones:

```python
from pipeline.load import load_csv
from pipeline.save import save_csv
from pipeline.transform import (
    clean_column_names, drop_nulls, filter_positive_values,
    drop_duplicates_by_column, normalize_column_0_100, uppercase_column
)

df = load_csv("data/input/ventas.csv")
df = clean_column_names(df)
df = drop_nulls(df)
df = filter_positive_values(df, "ventas")
df = drop_duplicates_by_column(df, "cliente")
df = normalize_column_0_100(df, "ventas")
df = uppercase_column(df, "cliente")
save_csv(df, "data/output/ventas_clean.csv")
```

* **Resultado**: pipeline funcional para un archivo fijo.

---

## **4ï¸âƒ£ Bloque 4 â€“ CLI con `argparse`**

Se reemplaza el main fijo por `main_ventas_cli.py` con **parÃ¡metros desde terminal**:

```python
import argparse
from pipeline.load import load_csv
from pipeline.save import save_csv
from pipeline.transform import (
    clean_column_names, drop_nulls, filter_positive_values,
    drop_duplicates_by_column, normalize_column_0_100, uppercase_column
)

def main(input_path, output_path, column, key_col, upper_col):
    df = load_csv(input_path)
    df = clean_column_names(df)
    df = drop_nulls(df)
    df = filter_positive_values(df, column)
    if key_col in df.columns:
        df = drop_duplicates_by_column(df, key_col)
    if column in df.columns:
        df = normalize_column_0_100(df, column)
    if upper_col in df.columns:
        df = uppercase_column(df, upper_col)
    save_csv(df, output_path)

if __name__ == "__main__":
    p = argparse.ArgumentParser(description="Pipeline ventas (CLI)")
    p.add_argument("--input", required=True, help="CSV de entrada")
    p.add_argument("--output", required=True, help="CSV de salida")
    p.add_argument("--column", default="ventas", help="Columna de valores positivos")
    p.add_argument("--key-col", default="cliente", help="Columna para quitar duplicados")
    p.add_argument("--upper-col", default="cliente", help="Columna para mayÃºsculas")
    args = p.parse_args()
    main(args.input, args.output, args.column, args.key_col, args.upper_col)
```

Ejemplo de ejecuciÃ³n:

```bash
python main_ventas_cli.py --input data/input/ventas.csv --output data/output/ventas_clean.csv
```

---

## **5ï¸âƒ£ Bloque 5 â€“ ParÃ¡metros opcionales**

* Se implementan **valores por defecto** en `argparse` para mayor flexibilidad.
* Permite ejecutar con:

```bash
python main_ventas_cli.py --input data/input/ventas.csv --output data/output/ventas_clean.csv --column ventas --key-col cliente
```

* Si no se pasan parÃ¡metros opcionales, usa valores por defecto definidos en `add_argument`.

---

## **6ï¸âƒ£ Bloque 6 â€“ Logging**

* Se agrega `utils/logger.py`:

```python
import logging
import os

def get_logger(name=__name__, log_file="logs/pipeline.log"):
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    fh = logging.FileHandler(log_file)
    formatter = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    return logger
```

* En `main_ventas_cli.py` se reemplazan los `print()` por:

```python
from utils.logger import get_logger
logger = get_logger()

logger.info("Inicio del pipeline")
...
logger.info(f"Archivo guardado en: {output_path}")
```

* **Ventaja**: los eventos quedan registrados en `logs/pipeline.log` para auditorÃ­a.

---

## **ğŸ“‚ Estructura final (hasta Bloque 6)**

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/ventas.csv
â”‚   â””â”€â”€ output/ventas_clean.csv
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline.log
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ load.py
â”‚   â”œâ”€â”€ save.py
â”‚   â”œâ”€â”€ transform.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ main_ventas_cli.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## **ğŸ“¦ InstalaciÃ³n y uso**

1. Crear y activar entorno virtual:

```bash
python -m venv venv
.\venv\Scripts\activate   # Windows
source venv/bin/activate  # Linux/Mac
```

2. Instalar dependencias:

```bash
pip install -r requirements.txt
```

3. Ejecutar pipeline:

```bash
python main_ventas_cli.py --input data/input/ventas.csv --output data/output/ventas_clean.csv
```

---

## **âœ… Resultado esperado**

En consola:

```
âœ… Archivo guardado exitosamente en: data/output/ventas_clean.csv
```

En `logs/pipeline.log`:

```
2025-08-10 15:30:01 | INFO | Inicio del pipeline
2025-08-10 15:30:01 | INFO | Cargando datos
2025-08-10 15:30:02 | INFO | Transformando datos
2025-08-10 15:30:02 | INFO | Guardando datos transformados
2025-08-10 15:30:02 | INFO | Pipeline finalizado con Ã©xito
```

---

